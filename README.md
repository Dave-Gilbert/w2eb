# W2EB

## W2EB Converts Wikipedia pages to offline ebooks

Wikipedia is a fantastic source of information which has changed the
way most people do research. However, Wikipedia pages are designed to
be viewed online. Usually you will need either a computer connected to
the Internet or a cell phone with a good data plan in order to access
Wikipedia.

E-ink readers have incredible battery life and easy to read screens
but they have several limitations in terms of what they can render. A
basic e-reader has a 6 inch 600x800 monochrome pixel display, small
memory storage, and performs poorly when used as a web browser.

There are many strategies for converting web content to an e-reader
format but the results are often unsatisfactory. This is where
**w2eb** comes in. The goal: take the rich online content found in a
Wikipedia article or in a wikibook, and convert it into a format
suitable for offline reading on a monochrome e-reader with a small
screen.

# W2EB Features

**W2eb** takes a topic name and converts it into a Wikipedia derived
ebook. It supports the following features:

1. **W2eb** follows *primary* links to create complete subsections.
  * *Primary* links are explored completely. All images and references
    are gathered and appended to the main text in subsections.
  *	*Primary* link selection is limited by the book title or user
    defined keywords. References become candidates for new *primary* links.
2. **W2eb** summarizes *secondary* links to create notes and footnotes.
  * Wikipedia articles include hundreds of references, **w2eb**
    presents these in summary form as *footnotes*.
  * Footnotes can be extended by *notes*, which provide an in-depth
    summary of an article.
3. **W2eb** organizes related wikipedia links into a single epub file.
  * A custom table of contents lists links to the original major
    headings and all subsections as well as all derived sections.
  * Footnotes, notes and other special indecies appear at the end.
4. **W2eb** is image friendly.
  * Wikipedia usually stores several versions of each image. **W2eb**
    finds the "best" one for your reader and fetches it.
5. **W2eb** is math friendly.
  * Equations get special treatment to ensure they are readable.

## Demo

In the demo directory there are a few sample epub books generated by
**w2eb** as well as some screenshots.

![TOC](https://github.com/Dave-Gilbert/w2eb/blob/master/demo/Aardvark_TOC_sm.jpg)
![Aardvark](https://github.com/Dave-Gilbert/w2eb/blob/master/demo/Aardvark_sm.jpg)
![Footnotes](https://github.com/Dave-Gilbert/w2eb/blob/master/demo/Aardvark_Foot_sm.jpg)

Wikipedia's Aardvark article: [https://en.wikipedia.org/wiki/Aardvark](https://en.wikipedia.org/wiki/Aardvark)

**W2EB**'s epub version: [Aardvark - w2eb.epub](https://github.com/Dave-Gilbert/w2eb/blob/master/demo/Aardvark%20-%20w2eb.epub)

**W2EB**'s mobi version: [Aardvark - w2eb.mobi](https://github.com/Dave-Gilbert/w2eb/blob/master/demo/Aardvark%20-%20w2eb.mobi)

## Why Wikipedia? - What about the *Rest* of the Web?

Wikipedia is an information rich source that presents pages in a
consistent way so that it is straight forward for a single tool to
re-interpret the data. Re-interpreting other data sources is fodder
for future work. Suggestions are welcome. The **w** in **w2eb** might
stand for *web* rather than *wiki*.

## Why this tool?

It is easy to find fiction in epub format. Project Gutenburg and
Archive.org are great sources for free books. If you want the latest
publications you can go to your local library. Despite all these many
good sources of fiction, it is very hard to find scientific or
mathematically based content formatted for an e-reader.

Project Gutenberg hosts only a small selection of math and science
books and these are in pdf and Latex format, not epub or html format.
Other web sites that provide science based media often limit their
offerings to pdf files. See for example the Openstax collection of
University freshman level math and science text books.

It is difficult to convert a pdf file to an epub file if the pdf file
includes equations, tables, unusual symbols or a complex page layouts.
The most interesting tool for this job is k2pdfopt, which effectively
treats each page of the pdf file as an image, cuts the image up, and
then rearranges the elements so the page can be viewed on the smaller
geometry of an e-reader's display. The k2pdfopt tool is great, but
it requires some tinkering to get the settings correct for each pdf
file, and once a file is converted fonts cannot be resized.

Some people say- "get a bigger tablet...", and yes this is an answer.
Depending on your needs this might be the right answer. A tablet with
a sufficiently high resolution and enough CPU cycles will allow you to
read whatever electronic file you want without using a desktop
computer. For me, the whole appeal of the e-ink reader is that it is
inexpensive, small, and highly portable. The trade-off incurred by
using a compact device is the imposed limitation that documents must
themselves be smaller and simpler in their presentation. **W2eb**
achieves this tradeoff for Wikipedia by simplifying page formatting as
much as possible without discarding content.

# Usage

**W2eb** is a command line tool with a variety of options. It is meant
to be simple to use, although the tool currently has several
dependencies so installation is not yet streamlined. Once the tool is
installed, lets suppose you want the Wikipedia book on Aardvarks. From
your command line you would type:

`$ w2eb -b aardvark`

**W2eb** tries this keywords with one of several Wikipedia base urls
and starts downloading text and images. **W2eb** generates several
progress meters with summary symbols so you have an idea of what it is
doing. When it is done it generates some summary statistics which
detail how many pages it visited.

Downloading the "aardvark" book generates the following output:

```plaintext
$ w2eb -b Aardvark


---------------------------------------------
==> Downloading Wikibook "aardvark"
Started at Tue Jul 16 14:57:31 2019
Searching: https://en.wikipedia.org/wiki/aardvark
Debug = 1, Depth = 1

Fixing Tags: id sc li me sp ti jl np na ta di td ft pf cl hi hi gn 

Collecting Figures (J/P)

 0 %  16 secs left > P J P P P J J J J J J J J P P P 

Collecting Footnotes (F), Articles (A), and Internet Links (I)

 0 % 112 secs left > * / F F F F F F F F F F F F F F F F F * F F F F F 
 7 %   3 mins left > F F F F F F F F F F f F F F F F F F F f F f f f F 
14 % 154 secs left > F F F F F F F F F F F F f F F f * F F f * F F F F 
22 % 136 secs left > F F F F F F F F F F F F F F F F F F F F F F F F F 
29 % 158 secs left > F F F F F F F F F F F F F F F F F F F F F F F F F 
36 % 159 secs left > F f f F F F F F F F F F f f f F F F F F * * F F F 
44 % 149 secs left > F F F F F F F F F F F F F F F F F F F F F F F F F 
51 % 134 secs left > F F F F F F F I i * F I F * F * F I I f I f * f I 
59 % 115 secs left > I F I I I * I I I I f I f I F / f / I F F f / I f 
66 %  93 secs left > / I I f I I F I f / F I f / f I I f / f / * I I I 
73 %  74 secs left > * I F I I F I I I F / F I f f f F F F F F F F F F 
81 %  57 secs left > F F F F F F F F F F F F F F f F F F F F F F F F F 
88 %  35 secs left > F F F F F F F F F F F I F I F I F I F I F I F I F 
96 %  12 secs left > I F I F I F I f I F I F I 
=================Finalizing==================

Found 16 Figures, Converted 16 Figures
Extracted 232 Footnotes.
Internet Refs = 283, Page Refs = 1258, Bad Links Removed = 10, http 404 = 11
Internet Accesses = 554, Cache hits = 79
Table of Contents Entries = 16

Finished at Tue Jul 16 15:02:38 2019
Conversion took   5 mins

wrote aardvark/aardvark.html
==> Done
```

## Performance

Using the default settings an article can be downloaded and processed
in about 1-5 minutes. Execution times vary widely depending on the
sort of internet connection you have, what article you are
downloading, and the extent to which you allow **w2eb** to probe
Wikipedia. Deep extractions will generate enormous ebooks which can be
unusable on an e-reader. **W2eb** generates a lot of cross references,
even for small articles, which when too numerous can cause e-readers
to crash.

A recursive breadth first search is used to explore a Wikipedia
article's reference structure. The depth of this search is limited by
the '-d' flag. References to web pages outside Wikipedia are verified,
but not explored. Most references within Wikipedia are treated
*secondary* and only summary information is collected. A *secondary*
reference is called either a note or a footnote in the ebook. A
*footnote* stores just the first few sentences from an article. If
*notes* are enabled, a separate section linked to the footnote will
provide a longer summary somewhere between 2 and 10 paragraphs.  Notes
include a link to the original article, so you can always review
Wikipedia's version.  Both notes and footnotes are quite simple, they
don't include images, lists, or tables. 

For a reference to be treated as a *primary* section it must have a
title similar to the current book's title, or include user specified
keywords in its title. This behaviour is controlled with the '-S'
flag. For example, while downloading the article on "Chernobyl",
several subsections are identified, including: "Chernobyl Disaster
Effects", "Chernobyl Disaster-Related Deaths", and "Cultural Impact Of
The Chernobyl Disaster". These wiki articles are treated as primary
articles because they share the name "Chernobyl" in their title and
these references are found in the main article. Increasing the search
depth to 2 or 3 via '-d' will allow more *primary* sections to be
identified from previously found *primary* sections. Ebooks will grow
exponentially in size, so care should be taken when selecting
parameters.

**W2eb** maintains a cache of everything it downloads. This was
originally implemented to reduce testing time although articles which
reference the same urls repeatedly also benefit from the cache.  After
a book is created for the first time, recreating it a second time with
different settings is much faster. Cache erasure is controlled by the
'-C', and '-K' flags.

Converting svg files into png files can be very time consuming,
especially for books which define a large number of equations. For the
kindle e-reader, svg files with transparent backgrounds are rendered
as black, so by default an svg image which does not appear to be a
math equation is converted to a png file. In my tests I did not see
any equation files that used transparent backgrounds so by default
equations are not converted. 

All footnotes and sections are merged into a single file which
requires **w2eb** to harmonize all of the cross referenced links. This
can take a while if the ebook being generated is very large and has a
lot of unresolved links.

## Options

**w2eb** provides a detailed usage message.

```plaintext
$ w2eb -h

W2EB - A tool for converting Wikipedia articles into ebooks.

    Usage:  w2eb.py  [opts] [-u <URL>] [-b <book>] 
        
        -C <#>    Clean cache, 1> failed urls + html files, 2> generated footnotes,
                  3> generated images, 4> generated equations  
        -K        Kleen cache, all generated files and all cached downloads.
        -E        Export book to epub. Uses calibre for conversion.
        
        -i        No images 
        -B        Convert color images to black and white.
        -P        Convert all .svg images to .png. Older e-readers may
                    not support .svg. Wikipedia provides math equations
                    in .svg format, although converting them is time
                    consuming.
        -s        Use .svg for non-math figures. Svg figures with
                    transparent zones don't render correctly on the kindle.
                    Default is to render them.

        -u <url>  Url to use as the base of the ebook. If -b is not supplied
                    the basename of the url will be used to generate the
                    bookname, usually these match.
        -b <nm>   The name of the e-book. Wikipedia urls are usually short
                    and use the url basename as a version of the article name.
                    If -u is not supplied, guess a url based on the bookname.
              
        -d <#>    depth, 0 for no subarticles. Default is 1.
        -n        Never include notes, only allow footnotes
        -N        Always allow notes to be generated alongside footnotes
        -S <typ>  Section type. Determines whether a link is treated
                    as a subsection or not.
                    
                    <typ> can be one of:
                    bookurl  - subsection url has book url as a substring
                    bookname - subsection url has book name as a substring
                               < default>
                    keyword:<key_1>,<key_2>,...<key_n> - subsection has any one
                               keys in the comma separated list as a substring
        
        -D <#>    Debug level. 0 = none, 1 = footnote only, 2 = failure only,
                    3 = all. Debug notes are included in the book by default.
        -w        Wiki down, rely on cache instead of wget (debugging...)
        -h        This message.

```

## Dependencies

**W2eb** is written for Python 2.7. The current version of **w2eb**
has several dependencies. HTML files are fetched by *wget* and
processed by *Beautiful Soup*, both must be present for the tool to
work. Images are processed by *Image Magick's* *"convert"* command
line tool. **W2eb** generates .html files that are epub friendly, but
still relies on third party tools to do the final conversion. **W2eb**
adds some tags that help *calibre* recognize headings that should be
included in the table of contents.  Testing has only been done on
Linux. 
 
Minimizing dependencies is important, as is the ability to execute on
various platforms. I hope to remove wget soon, and perhaps find a
reasonable Windows alternative to Image Magick's convert.

At this time there is no GUI.

## Documentation

**W2eb** uses http://epydoc.sourceforge.net/ for documenting functions.
Epydoc generates html documentation for python but also provides a well
structured mechanisms for laying out comments. See the html directory
for the epydoc output:

https://github.com/Dave-Gilbert/w2eb/tree/master/html_epydocs

These docs are paritailly rendered here:

http://htmlpreview.github.io/?https://github.com/Dave-Gilbert/w2eb/blob/master/html_epydocs/module-tree.html

# Bugs

Information is extracted from Wikipedia by crawling or scraping their
web pages. A better strategy would be to access their content database
directly.

**W2eb** tries to replace the table of contents in an article with a
corrected version, although depending on how the original table of
contents was created this doesn't always work. Sometimes there are
several table of contents, or **w2eb** puts its version in an odd
location.

**W2eb** Should generate a very brief page after the title page indicating how the ebook was made, citing the main Wikipedia references that were used as well as where to get w2eb. CLI options should appear at the end of the book along with a little more detail about w2eb.

**W2eb** adds hints to headings so that calibre will select
appropriate entries for the epub files native table of contents,
although this doesn't always work. Sometimes there are nonsense
TOC entries picked up by Calibre during import. 

**W2eb** generates notes and footnotes using the same heuristic which
drops all images, tables and formatting. This can sometimes leave
strange gaps in the footnote or note text.

**W2eb** rearranges text in certain situations. An e-reader presents
material in a more linear fashion than a web page. A Wikipedia page is
mostly linear, but includes side bars, large tables, and other
features. **W2eb**'s goal is to improve presentation through a simplified
arrangement, but this sometimes doesn't work quite right.

**W2eb** currently doesn't handle large tables in a very nice way.

**W2eb** only pulls the images out of Wikipedia's summary table at the
top of an article. This feature needs work.

**W2eb** relies on the presence of certain special tags which are
found only in .html pages generated by Wikipedia. This is fragile in
the sense that an upgrade to the wiki rendering system would break
**w2eb**, see the preceding note about reading Wikipedia's database directly.

It would be nice if **w2eb** generated a cover based on the book title and a few images from the book. Todo if time permits.

*The current version, as of Sept 2019, should be considered unstable and
not entirely ready for general consumption. While many core features work
correctly there are several important parts of Wikipedia articles that
are difficult to read and require better simplification.*


# License

**W2EB** is released under GPLv3. 

```
    W2EB is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
```

# Contact

Questions or comments are welcome.

dave.wm.gilbert@gmail.com
